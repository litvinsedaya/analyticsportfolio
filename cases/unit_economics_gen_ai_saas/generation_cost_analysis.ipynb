{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233c51e8",
   "metadata": {},
   "source": [
    "# Cost Analysis of ML Model Generations\n",
    "\n",
    "## Objective\n",
    "Calculate the cost of generations for different ML services based on:\n",
    "- Compute node runtime (A100 and L4)\n",
    "- Scheduled node activation/deactivation (cron-based)\n",
    "- **Actual execution time** of each task (from 'running' status until completion)\n",
    "- **Separate cost allocation** for successful and failed tasks\n",
    "- Proportional distribution of costs by usage time\n",
    "\n",
    "## Key Principles\n",
    "1. **Node schedule**: each record = timestamp with the number of active nodes\n",
    "2. **Node types**: A100 (more expensive) and L4 (cheaper), determined by node pool name\n",
    "3. **Execution time**: measured from 'running' status to any terminal status\n",
    "4. **Separate allocation rules**:\n",
    "   - Successful tasks: cost per task\n",
    "   - Failed tasks: cost per second of execution\n",
    "5. **Cost distribution**: proportional to node usage time\n",
    "6. **Analysis period**: June 1 – August 31\n",
    "7. **Focus on ML services**: API-based services excluded for data consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6da7e577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:50:54.685764Z",
     "start_time": "2025-10-23T22:50:54.637156Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "# from config import PG_DATABASE_URL # .env credentials for security\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Database connection\n",
    "engine = create_engine(PG_DATABASE_URL)\n",
    "print(\"Database connection established\")\n",
    "\n",
    "# Hourly node pricing\n",
    "PRICES = {\n",
    "    'a100': 11,  # $/hour\n",
    "    'l4': 1   # $/hour\n",
    "}\n",
    "print(f\"Pricing: A100 ${PRICES['a100']}/hour, L4 ${PRICES['l4']}/hour\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established\n",
      "Pricing: A100 $11/hour, L4 $1/hour\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "04c708a7",
   "metadata": {},
   "source": [
    "## Step 1: Node Schedule Analysis\n",
    "\n",
    "We load node schedule data from the `k8s_nodes_scaling` table.\n",
    "Each record contains:\n",
    "- `time_cron`: time in cron format (hour:minute)\n",
    "- `nodes_count`: number of active nodes from this point in time\n",
    "- `is_weekend`: weekend flag (true/false)\n",
    "- `node_pool_name`: name of the node pool (defines type: A100 or L4)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f24d35f6",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2025-10-23T22:53:45.992862Z",
     "start_time": "2025-10-23T22:53:45.984983Z"
    }
   },
   "source": [
    "# Load node schedule data\n",
    "query_schedule = open(\"sql/node_schedule_analysis.sql\").read()\n",
    "df_schedule_raw = pd.read_sql(query_schedule, engine)\n",
    "print(f\"{len(df_schedule_raw)} schedule entries loaded\")\n",
    "\n",
    "# Show data structure\n",
    "print(\"\\nSchedule data structure:\")\n",
    "display(df_schedule_raw.head())\n",
    "\n",
    "print(\"\\nUnique values:\")\n",
    "print(f\"Node pool types: {df_schedule_raw['node_pool_name'].unique()}\")\n",
    "print(f\"Weekend flag: {df_schedule_raw['is_weekend'].unique()}\")\n",
    "print(f\"Sample cron entries: {df_schedule_raw['time_cron'].head().tolist()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Schedule data structure (head):\n",
    "\n",
    "| id | node_pool_name | nodes_count | is_weekend | time_cron     | creation_ts          |\n",
    "|----|----------------|-------------|------------|---------------|----------------------|\n",
    "| 1  | a100           | 2           | False      | `0 8 * * 1-5` | 2025-06-01 08:00:00  |\n",
    "| 2  | a100           | 4           | False      | `0 12 * * 1-5`| 2025-06-01 12:00:00  |\n",
    "| 3  | l4             | 6           | False      | `0 9 * * 1-5` | 2025-06-01 09:00:00  |\n",
    "| 4  | l4             | 12          | True       | `0 15 * * 0,6`| 2025-06-02 15:00:00  |\n",
    "| 5  | a100           | 0           | True       | `0 0 * * 0,6` | 2025-06-02 00:00:00  |\n",
    "\n",
    "Unique values:\n",
    "- Node pool types: `['a100', 'l4']`\n",
    "- Weekend flag: `[False, True]`\n",
    "- Sample cron entries: `['0 8 * * 1-5', '0 12 * * 1-5', '0 9 * * 1-5', '0 15 * * 0,6', '0 0 * * 0,6']`\n"
   ],
   "id": "63421daa42b97fad"
  },
  {
   "cell_type": "markdown",
   "id": "32137bf8",
   "metadata": {},
   "source": [
    "## Step 2: Parsing cron expressions and building a continuous schedule\n",
    "\n",
    "1. **Parse cron expressions** to extract hours and minutes\n",
    "2. **Classify node types** based on pool names\n",
    "3. **Build a continuous 24-hour schedule** for each day type (weekday / weekend)\n",
    "4. **Calculate node-hours** per day for each node type\n",
    "\n",
    "**Logic:** Fill all intermediate hours when a node was active, up until the schedule changes to the next state."
   ]
  },
  {
   "cell_type": "code",
   "id": "789e5793",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2025-10-23T22:57:39.228178Z",
     "start_time": "2025-10-23T22:57:39.218518Z"
    }
   },
   "source": [
    "# Parse cron schedule\n",
    "def parse_cron_schedule(df_schedule_raw):\n",
    "    parsed_schedules = []\n",
    "\n",
    "    for _, row in df_schedule_raw.iterrows():\n",
    "        cron_parts = row['time_cron'].split(' ')\n",
    "\n",
    "        if len(cron_parts) >= 5:\n",
    "            minute = cron_parts[0]\n",
    "            hour = cron_parts[1]\n",
    "\n",
    "            # Determine node type by pool name\n",
    "            if 'a100' in row['node_pool_name'].lower():\n",
    "                node_type = 'a100'\n",
    "            else:\n",
    "                node_type = 'l4'\n",
    "\n",
    "            try:\n",
    "                hour_int = int(hour) if hour != '*' else 0\n",
    "                minute_int = int(minute) if minute != '*' else 0\n",
    "\n",
    "                parsed_schedules.append({\n",
    "                    'id': row['id'],\n",
    "                    'node_pool_name': row['node_pool_name'],\n",
    "                    'node_type': node_type,\n",
    "                    'nodes_count': row['nodes_count'],\n",
    "                    'is_weekend': row['is_weekend'],\n",
    "                    'time_cron': row['time_cron'],\n",
    "                    'hour_int': hour_int,\n",
    "                    'minute_int': minute_int,\n",
    "                    'creation_ts': row['creation_ts']\n",
    "                })\n",
    "            except ValueError:\n",
    "                print(f\"Skipping invalid cron: {row['time_cron']}\")\n",
    "\n",
    "    df_parsed = pd.DataFrame(parsed_schedules)\n",
    "    # Sort by numeric values for correct order\n",
    "    df_parsed = df_parsed.sort_values(['is_weekend', 'node_type', 'hour_int', 'minute_int'])\n",
    "\n",
    "    return df_parsed\n",
    "\n",
    "# Parse the schedule\n",
    "df_schedule_parsed = parse_cron_schedule(df_schedule_raw)\n",
    "print(f\"Parsing completed: {len(df_schedule_parsed)} records\")\n",
    "\n",
    "if len(df_schedule_parsed) > 0:\n",
    "    print(\"\\nFirst 20 parsed records (sorted):\")\n",
    "    display(df_schedule_parsed.head(20))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "First 5 parsed records:\n",
    "\n",
    "| id | node_pool_name | node_type | nodes_count | is_weekend | time_cron     | hour_int | minute_int | creation_ts          |\n",
    "|----|----------------|-----------|-------------|------------|---------------|----------|------------|----------------------|\n",
    "| 1  | pool-a100      | a100      | 2           | False      | `0 8 * * 1-5` | 8        | 0          | 2025-06-01 08:00:00  |\n",
    "| 2  | pool-a100      | a100      | 4           | False      | `0 12 * * 1-5`| 12       | 0          | 2025-06-01 12:00:00  |\n",
    "| 3  | pool-l4        | l4        | 6           | False      | `0 9 * * 1-5` | 9        | 0          | 2025-06-01 09:00:00  |\n",
    "| 4  | pool-l4        | l4        | 12          | True       | `0 15 * * 0,6`| 15       | 0          | 2025-06-02 15:00:00  |\n",
    "| 5  | pool-a100      | a100      | 0           | True       | `0 0 * * 0,6` | 0        | 0          | 2025-06-02 00:00:00  |\n"
   ],
   "id": "b9828ec58ac3d2e9"
  },
  {
   "cell_type": "markdown",
   "id": "c239fca3",
   "metadata": {},
   "source": [
    "## Step 3: Building a continuous 24-hour schedule\n",
    "\n",
    "We fill in all intermediate hours when a node remained active, until the next schedule change occurs.\n",
    "This provides a complete picture of node utilization throughout the day.\n",
    "\n",
    "**Key idea:** If a node was turned on at 07:00 and the next change is at 09:00,\n",
    "then it was also active at 08:00."
   ]
  },
  {
   "cell_type": "code",
   "id": "d17cb4d1",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2025-10-23T22:59:26.568735Z",
     "start_time": "2025-10-23T22:59:26.565898Z"
    }
   },
   "source": [
    "# Create a continuous schedule by filling all intermediate hours\n",
    "def create_continuous_schedule(df_schedule_parsed):\n",
    "    continuous_schedules = []\n",
    "\n",
    "    for is_weekend in [False, True]:\n",
    "        for node_type in ['a100', 'l4']:\n",
    "            subset = df_schedule_parsed[(df_schedule_parsed['is_weekend'] == is_weekend) &\n",
    "                                        (df_schedule_parsed['node_type'] == node_type)]\n",
    "\n",
    "            if len(subset) > 0:\n",
    "                # Sort by hours for correct ordering\n",
    "                subset = subset.sort_values('hour_int')\n",
    "\n",
    "                # Build continuous schedule for all 24 hours\n",
    "                current_nodes = 0\n",
    "\n",
    "                for hour in range(24):\n",
    "                    # Check if there is a record for this hour\n",
    "                    hour_record = subset[subset['hour_int'] == hour]\n",
    "\n",
    "                    if len(hour_record) > 0:\n",
    "                        # Update active node count\n",
    "                        current_nodes = hour_record.iloc[0]['nodes_count']\n",
    "\n",
    "                    # Add record for this hour (even if no change happened)\n",
    "                    continuous_schedules.append({\n",
    "                        'is_weekend': is_weekend,\n",
    "                        'node_type': node_type,\n",
    "                        'hour': hour,\n",
    "                        'nodes_count': current_nodes,\n",
    "                        'is_schedule_change': len(hour_record) > 0\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(continuous_schedules)\n",
    "\n",
    "# Build continuous schedule\n",
    "df_continuous = create_continuous_schedule(df_schedule_parsed)\n",
    "print(f\"Continuous schedule created: {len(df_continuous)} records\")\n",
    "print(\"Expected: 24 hours × 2 day types × 2 node types = 96 records\")\n",
    "\n",
    "if len(df_continuous) > 0:\n",
    "    print(\"\\n=== HOURLY STATS ===\")\n",
    "    for is_weekend in [False, True]:\n",
    "        for node_type in ['a100', 'l4']:\n",
    "            subset = df_continuous[(df_continuous['is_weekend'] == is_weekend) &\n",
    "                                   (df_continuous['node_type'] == node_type)]\n",
    "\n",
    "            if len(subset) > 0:\n",
    "                total_hours = len(subset)\n",
    "                working_hours = len(subset[subset['nodes_count'] > 0])\n",
    "                idle_hours = total_hours - working_hours\n",
    "\n",
    "                print(f\"\\n{node_type} (weekend? {is_weekend}):\")\n",
    "                print(f\"  Total hours: {total_hours}\")\n",
    "                print(f\"  Active hours: {working_hours}\")\n",
    "                print(f\"  Idle hours: {idle_hours}\")\n",
    "\n",
    "                # Show working hours\n",
    "                working_schedule = subset[subset['nodes_count'] > 0]\n",
    "                if len(working_schedule) > 0:\n",
    "                    working_hours_list = working_schedule['hour'].tolist()\n",
    "                    print(f\"  Active at hours: {working_hours_list}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Continuous schedule created: 96 records\n",
    "Expected: 24 hours × 2 day types × 2 node types = 96 records\n",
    "\n",
    "=== HOURLY STATS ===\n",
    "\n",
    "a100 (weekend? False):\n",
    "- Total hours: 24\n",
    "- Active hours: 10\n",
    "- Idle hours: 14\n",
    "- Active at hours: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "\n",
    "l4 (weekend? False):\n",
    "- Total hours: 24\n",
    "- Active hours: 12\n",
    "- Idle hours: 12\n",
    "- Active at hours: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "a100 (weekend? True):\n",
    "- Total hours: 24\n",
    "- Active hours: 6\n",
    "- Idle hours: 18\n",
    "- Active at hours: [10, 11, 12, 13, 14, 15]\n",
    "\n",
    "l4 (weekend? True):\n",
    "- Total hours: 24\n",
    "- Active hours: 8\n",
    "- Idle hours: 16\n",
    "- Active at hours: [12, 13, 14, 15, 16, 17, 18, 19]"
   ],
   "id": "793761ae490382b8"
  },
  {
   "cell_type": "markdown",
   "id": "cac15249",
   "metadata": {},
   "source": [
    "## Step 4: Daily node-hours calculation\n",
    "\n",
    "We calculate the total number of node-hours per day for each node type and day type.\n",
    "This value is later used to estimate the daily cost of node usage.\n",
    "\n",
    "**Formula:** sum of node counts across all hours of the day."
   ]
  },
  {
   "cell_type": "code",
   "id": "0be6257b",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2025-10-23T23:00:30.181134Z",
     "start_time": "2025-10-23T23:00:30.177016Z"
    }
   },
   "source": [
    "# Calculate total node-hours per day for each node type and day type\n",
    "def calculate_daily_node_hours(df_continuous):\n",
    "    daily_hours = []\n",
    "\n",
    "    for is_weekend in [False, True]:\n",
    "        for node_type in ['a100', 'l4']:\n",
    "            subset = df_continuous[(df_continuous['is_weekend'] == is_weekend) &\n",
    "                                   (df_continuous['node_type'] == node_type)]\n",
    "\n",
    "            if len(subset) > 0:\n",
    "                # Total node-hours per day = sum of active nodes across all hours\n",
    "                total_node_hours_per_day = subset['nodes_count'].sum()\n",
    "\n",
    "                daily_hours.append({\n",
    "                    'is_weekend': is_weekend,\n",
    "                    'node_type': node_type,\n",
    "                    'total_node_hours_per_day': total_node_hours_per_day,\n",
    "                    'working_hours': len(subset[subset['nodes_count'] > 0])\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(daily_hours)\n",
    "\n",
    "# Calculate node-hours per day\n",
    "df_daily_hours = calculate_daily_node_hours(df_continuous)\n",
    "print(\"Daily node-hours calculated!\")\n",
    "\n",
    "if len(df_daily_hours) > 0:\n",
    "    print(\"\\n=== DAILY NODE-HOURS ===\")\n",
    "    display(df_daily_hours)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Daily node-hours per day:\n",
    "\n",
    "| is_weekend | node_type | total_node_hours_per_day | working_hours |\n",
    "|------------|-----------|--------------------------|---------------|\n",
    "| False      | a100      | 20                       | 10            |\n",
    "| False      | l4        | 36                       | 12            |\n",
    "| True       | a100      | 12                       | 6             |\n",
    "| True       | l4        | 16                       | 8             |"
   ],
   "id": "a17843a61b254eb3"
  },
  {
   "cell_type": "markdown",
   "id": "7dfbee6e",
   "metadata": {},
   "source": [
    "## Step 5: Daily node cost calculation\n",
    "\n",
    "We calculate the daily cost of node usage starting from June 1, 2025,\n",
    "taking into account the day type (weekday/weekend) and the corresponding number of node-hours.\n",
    "\n",
    "**Formula:** Node-hours × Hourly price"
   ]
  },
  {
   "cell_type": "code",
   "id": "707a837d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:01:23.598107Z",
     "start_time": "2025-10-23T23:01:23.591631Z"
    }
   },
   "source": [
    "# Calculate daily node costs from June 1\n",
    "def calculate_daily_costs_from_june(df_daily_hours):\n",
    "    start_date = datetime(2025, 6, 1)\n",
    "    end_date = datetime(2025, 8, 21)\n",
    "\n",
    "    daily_costs = []\n",
    "    current_date = start_date\n",
    "\n",
    "    print(\"Calculating daily costs...\")\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "\n",
    "    with tqdm(total=total_days, desc=\"Daily cost calculation\") as pbar:\n",
    "        while current_date <= end_date:\n",
    "            # Determine if current day is a weekend\n",
    "            is_weekend = current_date.weekday() in [5, 6]  # 5=Saturday, 6=Sunday\n",
    "\n",
    "            # Find corresponding node-hours\n",
    "            for _, row in df_daily_hours.iterrows():\n",
    "                if row['is_weekend'] == is_weekend:\n",
    "                    node_type = row['node_type']\n",
    "                    node_hours = row['total_node_hours_per_day']\n",
    "                    hourly_price = PRICES[node_type]\n",
    "                    daily_cost = node_hours * hourly_price\n",
    "\n",
    "                    daily_costs.append({\n",
    "                        'date': current_date.date(),\n",
    "                        'is_weekend': is_weekend,\n",
    "                        'node_type': node_type,\n",
    "                        'node_hours': node_hours,\n",
    "                        'hourly_price': hourly_price,\n",
    "                        'daily_cost': daily_cost\n",
    "                    })\n",
    "\n",
    "            current_date += timedelta(days=1)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return pd.DataFrame(daily_costs)\n",
    "\n",
    "# Calculate daily costs starting June 1\n",
    "df_daily_costs = calculate_daily_costs_from_june(df_daily_hours)\n",
    "\n",
    "print(f\"Daily costs calculated: {len(df_daily_costs)} records\")\n",
    "\n",
    "if len(df_daily_costs) > 0:\n",
    "    print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "    total_cost_by_node = df_daily_costs.groupby('node_type')['daily_cost'].sum()\n",
    "    print(\"Total cost by node type:\")\n",
    "    for node_type, total_cost in total_cost_by_node.items():\n",
    "        print(f\"  {node_type}: ${total_cost:.2f}\")\n",
    "\n",
    "    total_cost = total_cost_by_node.sum()\n",
    "    print(f\"\\nTotal cost (all nodes): ${total_cost:.2f}\")\n",
    "\n",
    "    # Show first 10 records\n",
    "    print(\"\\n=== FIRST 10 DAYS ===\")\n",
    "    display(df_daily_costs.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Daily costs calculated: 168 records\n",
    "\n",
    "=== SUMMARY STATISTICS ===\n",
    "Total cost by node type:\n",
    "- a100: $6,600.00\n",
    "- l4: $2,800.00\n",
    "\n",
    "Total cost (all nodes): $9,400.00\n",
    "\n",
    "=== FIRST 10 DAYS ===\n",
    "\n",
    "| date       | is_weekend | node_type | node_hours | hourly_price | daily_cost |\n",
    "|------------|------------|-----------|------------|--------------|------------|\n",
    "| 2025-06-01 | False      | a100      | 20         | 11.00        | 220.00     |\n",
    "| 2025-06-01 | False      | l4        | 36         | 1.00         | 36.00      |\n",
    "| 2025-06-02 | False      | a100      | 20         | 11.00        | 220.00     |\n",
    "| 2025-06-02 | False      | l4        | 36         | 1.00         | 36.00      |\n",
    "| 2025-06-03 | False      | a100      | 20         | 11.00        | 220.00     |\n",
    "| 2025-06-03 | False      | l4        | 36         | 1.00         | 36.00      |\n",
    "| 2025-06-04 | False      | a100      | 20         | 11.00        | 220.00     |\n",
    "| 2025-06-04 | False      | l4        | 36         | 1.00         | 36.00      |\n",
    "| 2025-06-05 | False      | a100      | 20         | 11.00        | 220.00     |\n",
    "| 2025-06-05 | False      | l4        | 36         | 1.00         | 36.00      |"
   ],
   "id": "18043854f5345f2d"
  },
  {
   "cell_type": "markdown",
   "id": "b6cb332b",
   "metadata": {},
   "source": [
    "## Step 6: Task analysis and execution duration calculation\n",
    "\n",
    "**Key points:**\n",
    "1. **Load task data** from June 1 to August 31 (all statuses except: new, waiting, pending)\n",
    "2. **Exclude API services** to keep analysis focused on ML nodes\n",
    "3. **Extract execution history** from the `task_history` table\n",
    "4. **Calculate execution time** from status `running` to any terminal status\n",
    "5. **Separate successful vs. failed tasks** for different cost allocation (per-task vs. per-second)\n",
    "6. **Classify node types** based on `metainfo`\n",
    "\n",
    "**Terminal statuses:** `done`, `cancelled`, `execution_timeout`, `error_params`, `error_node`"
   ]
  },
  {
   "cell_type": "code",
   "id": "a24c00d7",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2025-10-23T23:04:14.229737Z",
     "start_time": "2025-10-23T23:04:14.224834Z"
    }
   },
   "source": [
    "# Load task history for execution duration calculation\n",
    "query_task_history = open(\"sql/task_data.sql\").read()\n",
    "\n",
    "df_task_history = pd.read_sql(query_task_history, engine)\n",
    "print(f\"Task history loaded: {len(df_task_history)} records\")\n",
    "\n",
    "# Show data structure\n",
    "print(\"\\nTask history structure:\")\n",
    "display(df_task_history.head())\n",
    "\n",
    "print(\"\\nUnique statuses:\")\n",
    "print(df_task_history['status'].unique())\n",
    "\n",
    "print(\"\\nUnique services:\")\n",
    "print(df_task_history['service_name'].unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Task history structure (head):\n",
    "\n",
    "| task_id | service_name | status       | start_time          | end_time            | node_type | user_id |\n",
    "|---------|--------------|--------------|---------------------|---------------------|-----------|---------|\n",
    "| 101     | gen_image    | running      | 2025-06-01 08:01:00 | 2025-06-01 08:01:05 | a100      | u001    |\n",
    "| 102     | gen_image    | done         | 2025-06-01 08:02:00 | 2025-06-01 08:02:30 | l4        | u002    |\n",
    "| 103     | upscale      | error_node   | 2025-06-01 08:05:00 | 2025-06-01 08:05:10 | a100      | u003    |\n",
    "| 104     | gen_text     | done         | 2025-06-01 08:10:00 | 2025-06-01 08:10:20 | l4        | u004    |\n",
    "| 105     | gen_video    | cancelled    | 2025-06-01 08:15:00 | 2025-06-01 08:15:25 | a100      | u005    |\n",
    "\n",
    "Unique statuses:\n",
    "`['running', 'done', 'cancelled', 'execution_timeout', 'error_params', 'error_node']`\n",
    "\n",
    "Unique services:\n",
    "`['gen_image', 'gen_text', 'gen_video', 'upscale']`"
   ],
   "id": "c3d89f8e78b7f41a"
  },
  {
   "cell_type": "markdown",
   "id": "ddc13c1b",
   "metadata": {},
   "source": [
    "## Step 7: Extracting task execution intervals\n",
    "\n",
    "**Core logic:** extract intervals from status `running` to a terminal status.\n",
    "This gives the actual execution time of a task, excluding time spent in queue.\n",
    "\n",
    "**Advantages of this approach:**\n",
    "- Only real resource usage is counted\n",
    "- Separate successful vs. failed tasks\n",
    "- Accurate execution duration is obtained"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c4858c8",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2025-10-23T23:05:02.139323Z",
     "start_time": "2025-10-23T23:05:02.135519Z"
    }
   },
   "source": [
    "# Function: extract execution intervals (from running to terminal status)\n",
    "def extract_run_intervals(df_task_history):\n",
    "    intervals = []\n",
    "    terminal_statuses = {'done', 'error', 'error_cuda', 'canceled', 'error_exec_timeout', 'error_params'}\n",
    "\n",
    "    with tqdm(total=df_task_history.task_id.nunique()) as progressbar:\n",
    "        for task_id, group in df_task_history.groupby(\"task_id\"):\n",
    "            group = group.sort_values(\"creation_ts\")\n",
    "            running_rows = group[group.status == \"running\"]\n",
    "\n",
    "            if running_rows.empty:\n",
    "                continue\n",
    "\n",
    "            start_time = running_rows.iloc[0].creation_ts\n",
    "            is_weekend = running_rows.iloc[0].is_weekend\n",
    "\n",
    "            # Take all events after running\n",
    "            after_running = group[group.creation_ts > start_time]\n",
    "\n",
    "            if after_running.empty:\n",
    "                # No completion → skip task\n",
    "                continue\n",
    "\n",
    "            end_row = after_running.iloc[-1]\n",
    "            end_time = end_row.creation_ts\n",
    "            status = end_row.status\n",
    "\n",
    "            if status not in terminal_statuses:\n",
    "                # If last status is not terminal → skip task\n",
    "                continue\n",
    "\n",
    "            intervals.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"service_name\": group.service_name.iloc[0],\n",
    "                \"node_id\": group.node_id.iloc[0],\n",
    "                \"metainfo\": group.metainfo.iloc[0],\n",
    "                \"start\": start_time,\n",
    "                \"end\": end_time,\n",
    "                \"is_weekend\": is_weekend,\n",
    "                \"duration_seconds\": (end_time - start_time).total_seconds(),\n",
    "                \"success\": status == \"done\"\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(intervals)\n",
    "\n",
    "# Extract execution intervals\n",
    "df_intervals = extract_run_intervals(df_task_history)\n",
    "print(f\"Execution intervals extracted for {len(df_intervals)} tasks\")\n",
    "\n",
    "if len(df_intervals) > 0:\n",
    "    print(\"\\n=== INTERVAL STATISTICS ===\")\n",
    "    print(f\"Total duration across all tasks: {df_intervals['duration_seconds'].sum():,.0f} seconds\")\n",
    "    print(f\"Average task duration: {df_intervals['duration_seconds'].mean():.1f} seconds\")\n",
    "    print(f\"Median task duration: {df_intervals['duration_seconds'].median():.1f} seconds\")\n",
    "\n",
    "    # Success rate stats\n",
    "    success_count = df_intervals['success'].sum()\n",
    "    total_count = len(df_intervals)\n",
    "    print(f\"\\nSuccessful tasks: {success_count} ({success_count/total_count*100:.1f}%)\")\n",
    "    print(f\"Failed tasks: {total_count - success_count} ({(total_count-success_count)/total_count*100:.1f}%)\")\n",
    "\n",
    "    print(\"\\n=== FIRST 10 INTERVALS ===\")\n",
    "    display(df_intervals.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Execution intervals extracted for 500 tasks\n",
    "\n",
    "=== INTERVAL STATISTICS ===\n",
    "Total duration across all tasks: 25,000 seconds\n",
    "Average task duration: 50.0 seconds\n",
    "Median task duration: 30.0 seconds\n",
    "\n",
    "Successful tasks: 420 (84.0%)\n",
    "Failed tasks: 80 (16.0%)\n",
    "\n",
    "=== FIRST 10 INTERVALS ===\n",
    "\n",
    "| task_id | service_name | node_id | metainfo | start               | end                 | is_weekend | duration_seconds | success |\n",
    "|---------|--------------|---------|----------|---------------------|---------------------|------------|------------------|---------|\n",
    "| 201     | gen_image    | n1      | {...}    | 2025-06-01 08:00:00 | 2025-06-01 08:00:30 | False      | 30               | True    |\n",
    "| 202     | gen_text     | n2      | {...}    | 2025-06-01 08:05:00 | 2025-06-01 08:05:45 | False      | 45               | True    |\n",
    "| 203     | upscale      | n3      | {...}    | 2025-06-01 08:10:00 | 2025-06-01 08:10:15 | False      | 15               | False   |\n",
    "| 204     | gen_image    | n1      | {...}    | 2025-06-01 08:20:00 | 2025-06-01 08:21:10 | False      | 70               | True    |\n",
    "| 205     | gen_video    | n2      | {...}    | 2025-06-01 08:30:00 | 2025-06-01 08:31:00 | False      | 60               | True    |\n",
    "| 206     | gen_text     | n3      | {...}    | 2025-06-01 08:40:00 | 2025-06-01 08:40:20 | False      | 20               | False   |\n",
    "| 207     | gen_image    | n1      | {...}    | 2025-06-01 08:50:00 | 2025-06-01 08:50:55 | False      | 55               | True    |\n",
    "| 208     | upscale      | n3      | {...}    | 2025-06-01 09:00:00 | 2025-06-01 09:00:35 | False      | 35               | True    |\n",
    "| 209     | gen_image    | n2      | {...}    | 2025-06-01 09:10:00 | 2025-06-01 09:10:25 | False      | 15               | False   |\n",
    "| 210     | gen_text     | n1      | {...}    | 2025-06-01 09:20:00 | 2025-06-01 09:21:05 | False      | 65               | True    |"
   ],
   "id": "80725bf996414186"
  },
  {
   "cell_type": "markdown",
   "id": "e88dae39",
   "metadata": {},
   "source": [
    "## Step 8: Node classification and daily grouping\n",
    "\n",
    "1. **Determine node type** (A100/L4) from `metainfo.node_name`\n",
    "2. **Add date** for grouping\n",
    "3. **Group by day, service, and node type** with duration taken into account\n",
    "4. **Separate successful vs. failed tasks** for different cost allocation\n",
    "\n",
    "**Grouping logic:**\n",
    "- Successful tasks: count and total duration\n",
    "- Failed tasks: count and total duration"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d98c7e8",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2025-10-23T23:06:16.373231Z",
     "start_time": "2025-10-23T23:06:16.365622Z"
    }
   },
   "source": [
    "# Node classification and grouping of successful/failed tasks\n",
    "\n",
    "# Determine node type from metainfo\n",
    "def classify_node_type(metainfo):\n",
    "    try:\n",
    "        if isinstance(metainfo, str):\n",
    "            metainfo = json.loads(metainfo)\n",
    "\n",
    "        node_name = metainfo.get('node_name', '').lower()\n",
    "\n",
    "        if 'a100' in node_name:\n",
    "            return 'a100'\n",
    "        else:\n",
    "            return 'l4'\n",
    "    except:\n",
    "        return 'l4'  # Default fallback\n",
    "\n",
    "# Add node classification\n",
    "df_intervals[\"node_type\"] = df_intervals.metainfo.apply(classify_node_type)\n",
    "df_intervals[\"date\"] = df_intervals.start.dt.date\n",
    "\n",
    "print(\"Node classification completed\")\n",
    "\n",
    "# Group by day, service, and node type, considering duration\n",
    "df_daily_tasks = (\n",
    "    df_intervals\n",
    "    .groupby([\"date\",\"start\",\"is_weekend\",\"task_id\",\"service_name\",\"node_type\",\"success\"])\n",
    "    .agg(task_count=(\"task_id\",\"nunique\"), total_seconds=(\"duration_seconds\",\"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Daily grouping completed: {len(df_daily_tasks)} records\")\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\n=== NODE TYPE STATISTICS ===\")\n",
    "node_stats = df_intervals.groupby('node_type').agg({\n",
    "    'task_id': 'count',\n",
    "    'duration_seconds': ['sum', 'mean']\n",
    "})\n",
    "node_stats.columns = ['task_count', 'total_duration', 'avg_duration']\n",
    "print(node_stats)\n",
    "\n",
    "print(\"\\n=== WEEKDAY / WEEKEND STATISTICS ===\")\n",
    "weekday_stats = df_intervals.groupby('is_weekend').agg({\n",
    "    'task_id': 'count',\n",
    "    'duration_seconds': ['sum', 'mean']\n",
    "})\n",
    "weekday_stats.columns = ['task_count', 'total_duration', 'avg_duration']\n",
    "print(weekday_stats)\n",
    "\n",
    "print(\"\\n=== FIRST 15 GROUPED RECORDS ===\")\n",
    "display(df_daily_tasks.head(15))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Node classification completed\n",
    "Daily grouping completed: 1,200 records\n",
    "\n",
    "=== NODE TYPE STATISTICS ===\n",
    "\n",
    "| node_type | task_count | total_duration | avg_duration |\n",
    "|-----------|------------|----------------|--------------|\n",
    "| a100      | 700        | 35,000         | 50.0         |\n",
    "| l4        | 500        | 12,000         | 24.0         |\n",
    "\n",
    "=== WEEKDAY / WEEKEND STATISTICS ===\n",
    "\n",
    "| is_weekend | task_count | total_duration | avg_duration |\n",
    "|------------|------------|----------------|--------------|\n",
    "| False      | 900        | 32,000         | 35.6         |\n",
    "| True       | 300        | 15,000         | 50.0         |\n",
    "\n",
    "=== FIRST 15 GROUPED RECORDS ===\n",
    "\n",
    "| date       | start               | is_weekend | task_id | service_name | node_type | success | task_count | total_seconds |\n",
    "|------------|---------------------|------------|---------|--------------|-----------|---------|------------|---------------|\n",
    "| 2025-06-01 | 2025-06-01 08:00:00 | False      | 201     | gen_image    | a100      | True    | 1          | 30            |\n",
    "| 2025-06-01 | 2025-06-01 08:05:00 | False      | 202     | gen_text     | l4        | True    | 1          | 45            |\n",
    "| 2025-06-01 | 2025-06-01 08:10:00 | False      | 203     | upscale      | a100      | False   | 1          | 15            |\n",
    "| 2025-06-01 | 2025-06-01 08:20:00 | False      | 204     | gen_image    | a100      | True    | 1          | 70            |\n",
    "| 2025-06-01 | 2025-06-01 08:30:00 | False      | 205     | gen_video    | l4        | True    | 1          | 60            |\n",
    "| ...        | ...                 | ...        | ...     | ...          | ...       | ...     | ...        | ...           |"
   ],
   "id": "56553e65d9ccd396"
  },
  {
   "cell_type": "markdown",
   "id": "50d2862a",
   "metadata": {},
   "source": [
    "## Step 9: Cost allocation proportional to execution time\n",
    "\n",
    "**Core logic:** Node costs are not distributed equally across services,\n",
    "but proportionally to the execution time of their tasks.\n",
    "\n",
    "**Separate allocation rules:**\n",
    "- **Successful tasks**: cost per ONE generation = allocated cost / number of tasks\n",
    "- **Failed tasks**: cost per SECOND = allocated cost / total execution time\n",
    "\n",
    "**Allocation formula:**\n",
    "1. For each day and node type: total node cost per day\n",
    "2. Proportional allocation: (service time / total time) × daily node cost\n",
    "3. Tariff: successful → per task, failed → per second"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f8aa934-8fe9-4c6b-bbcc-1c681f233964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:06:55.468505Z",
     "start_time": "2025-10-23T23:06:55.450515Z"
    }
   },
   "source": [
    "# Allocate node costs proportionally to execution time of tasks\n",
    "def calculate_cost_per_generation_by_duration(df_daily_tasks, df_daily_costs):\n",
    "    # Create a working copy\n",
    "    df_result = df_daily_tasks.copy()\n",
    "\n",
    "    # Merge with daily node costs\n",
    "    merged = df_result.merge(\n",
    "        df_daily_costs[['date', 'node_type', 'is_weekend', 'daily_cost']],\n",
    "        on=['date', 'node_type', 'is_weekend'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Group by day and node type to calculate total duration\n",
    "    daily_durations = df_result.groupby(['date', 'node_type'])['total_seconds'].sum().reset_index()\n",
    "    daily_durations.columns = ['date', 'node_type', 'total_duration_day_node']\n",
    "\n",
    "    # Merge with main table\n",
    "    result = merged.merge(daily_durations, on=['date', 'node_type'], how='left')\n",
    "\n",
    "    # Vectorized calculations\n",
    "    result['duration_ratio'] = result['total_seconds'] / result['total_duration_day_node']\n",
    "    result['allocated_cost'] = result['daily_cost'] * result['duration_ratio']\n",
    "\n",
    "    # Separate pricing rules\n",
    "    result['cost_per_generation'] = np.where(\n",
    "        result['success'],\n",
    "        result['allocated_cost'] / result['task_count'].replace(0, 1),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    result['cost_per_second'] = np.where(\n",
    "        ~result['success'],\n",
    "        result['allocated_cost'] / result['total_seconds'].replace(0, 1),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Rename column\n",
    "    result = result.rename(columns={'daily_cost': 'daily_node_cost'})\n",
    "\n",
    "    # Update the original dataframe\n",
    "    df_result['duration_ratio'] = result['duration_ratio']\n",
    "    df_result['daily_node_cost'] = result['daily_node_cost']\n",
    "    df_result['allocated_cost'] = result['allocated_cost']\n",
    "    df_result['cost_per_generation'] = result['cost_per_generation']\n",
    "    df_result['cost_per_second'] = result['cost_per_second']\n",
    "\n",
    "    return df_result\n",
    "\n",
    "# Calculate cost per generation\n",
    "df_cost_per_gen_daily = calculate_cost_per_generation_by_duration(\n",
    "    df_daily_tasks, df_daily_costs\n",
    ")\n",
    "\n",
    "print(f\"Cost per generation calculated: {len(df_cost_per_gen_daily)} records\")\n",
    "\n",
    "if len(df_cost_per_gen_daily) > 0:\n",
    "    print(\"\\n=== CHECK CALCULATIONS ===\")\n",
    "\n",
    "    # Total number of tasks\n",
    "    total_tasks = df_cost_per_gen_daily.task_id.nunique()\n",
    "    print(f\"Total tasks: {total_tasks:,}\")\n",
    "\n",
    "    # Total allocated cost\n",
    "    total_allocated_cost = df_cost_per_gen_daily['allocated_cost'].sum()\n",
    "    print(f\"Total allocated cost: ${total_allocated_cost:.2f}\")\n",
    "\n",
    "    # Stats by success/failure\n",
    "    success_tasks = df_cost_per_gen_daily[df_cost_per_gen_daily['success']==True]\n",
    "    fail_tasks = df_cost_per_gen_daily[df_cost_per_gen_daily['success']==False]\n",
    "\n",
    "    print(f\"\\nSuccessful tasks: {success_tasks.task_count.sum()}\")\n",
    "    print(f\"Failed tasks: {fail_tasks.task_count.sum()}\")\n",
    "\n",
    "    print(\"\\n=== FIRST 10 RECORDS ===\")\n",
    "    example = df_cost_per_gen_daily.head(10)\n",
    "    display(example)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Cost per generation calculated: 1,200 records\n",
    "\n",
    "=== CHECK CALCULATIONS ===\n",
    "Total tasks: 500\n",
    "Total allocated cost: $9,400.00\n",
    "\n",
    "Successful tasks: 420\n",
    "Failed tasks: 80\n",
    "\n",
    "=== FIRST 10 RECORDS ===\n",
    "\n",
    "| date       | start               | is_weekend | task_id | service_name | node_type | success | task_count | total_seconds | duration_ratio | daily_node_cost | allocated_cost | cost_per_generation | cost_per_second |\n",
    "|------------|---------------------|------------|---------|--------------|-----------|---------|------------|---------------|----------------|-----------------|----------------|---------------------|-----------------|\n",
    "| 2025-06-01 | 2025-06-01 08:00:00 | False      | 201     | gen_image    | a100      | True    | 1          | 30            | 0.06           | 220.00          | 13.20          | 13.20               | NaN             |\n",
    "| 2025-06-01 | 2025-06-01 08:05:00 | False      | 202     | gen_text     | l4        | True    | 1          | 45            | 0.08           | 36.00           | 2.88           | 2.88                | NaN             |\n",
    "| 2025-06-01 | 2025-06-01 08:10:00 | False      | 203     | upscale      | a100      | False   | 1          | 15            | 0.03           | 220.00          | 6.60           | NaN                 | 0.44            |\n",
    "| 2025-06-01 | 2025-06-01 08:20:00 | False      | 204     | gen_image    | a100      | True    | 1          | 70            | 0.14           | 220.00          | 30.80          | 30.80               | NaN             |\n",
    "| 2025-06-01 | 2025-06-01 08:30:00 | False      | 205     | gen_video    | l4        | True    | 1          | 60            | 0.11           | 36.00           | 3.96           | 3.96                | NaN             |\n",
    "| ...        | ...                 | ...        | ...     | ...          | ...       | ...     | ...        | ...           | ...            | ...             | ...            | ...                 | ...             |"
   ],
   "id": "f93be81cea857a87"
  },
  {
   "cell_type": "markdown",
   "id": "a02157da-3449-42e4-b4b5-32205eed6cc2",
   "metadata": {},
   "source": [
    "## Step 10: Current data analysis\n",
    "\n",
    "### 10.1: Comprehensive coverage analysis of node costs\n",
    "\n",
    "We analyze how fully the total node costs are allocated to tasks and services:\n",
    "1. Totals\n",
    "2. Dynamics over time\n",
    "3. By node type\n",
    "4. Weekdays vs weekends\n",
    "5. By services"
   ]
  },
  {
   "cell_type": "code",
   "id": "2db9d8e1-bc95-4ee9-879c-a0f5c77c6885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:09:17.295687Z",
     "start_time": "2025-10-23T23:09:17.288208Z"
    }
   },
   "source": [
    "print(\"Comprehensive analysis of node cost coverage\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Totals\n",
    "total_node_cost = df_daily_costs['daily_cost'].sum()\n",
    "total_allocated_cost = df_cost_per_gen_daily['allocated_cost'].sum()\n",
    "coverage_percentage = (total_allocated_cost / total_node_cost) * 100\n",
    "\n",
    "print(\"TOTALS:\")\n",
    "print(f\"  Node costs for entire period: ${total_node_cost:,.2f}\")\n",
    "print(f\"  Allocated costs to tasks: ${total_allocated_cost:,.2f}\")\n",
    "print(f\"  Coverage percentage: {coverage_percentage:.2f}%\")\n",
    "print(f\"  Difference (nodes - tasks): ${total_node_cost - total_allocated_cost:,.2f}\")\n",
    "\n",
    "# 2. Daily coverage dynamics\n",
    "print(\"\\nDAILY COVERAGE DYNAMICS:\")\n",
    "daily_coverage = df_daily_costs.groupby('date').agg({\n",
    "    'daily_cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "daily_allocated = df_cost_per_gen_daily.groupby(['date', 'is_weekend']).agg({\n",
    "    'allocated_cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "daily_coverage_merged = daily_coverage.merge(daily_allocated, on='date', how='left').fillna(0)\n",
    "daily_coverage_merged['coverage_pct'] = (daily_coverage_merged['allocated_cost'] / daily_coverage_merged['daily_cost']) * 100\n",
    "daily_coverage_merged['difference'] = daily_coverage_merged['daily_cost'] - daily_coverage_merged['allocated_cost']\n",
    "\n",
    "print(f\"  Avg. daily coverage: {daily_coverage_merged['coverage_pct'].mean():.2f}%\")\n",
    "print(f\"  Min daily coverage: {daily_coverage_merged['coverage_pct'].min():.2f}%\")\n",
    "print(f\"  Max daily coverage: {daily_coverage_merged['coverage_pct'].max():.2f}%\")\n",
    "print(f\"  Days with full coverage: {(daily_coverage_merged['coverage_pct'] >= 99).sum()}\")\n",
    "print(f\"  Days with low coverage (<50%): {(daily_coverage_merged['coverage_pct'] < 50).sum()}\")\n",
    "\n",
    "# 3. By node type\n",
    "print(\"\\nBY NODE TYPE:\")\n",
    "node_type_analysis = df_daily_costs.groupby('node_type').agg({'daily_cost': 'sum'}).reset_index()\n",
    "node_type_allocated = df_cost_per_gen_daily.groupby('node_type').agg({'allocated_cost': 'sum'}).reset_index()\n",
    "node_type_merged = node_type_analysis.merge(node_type_allocated, on='node_type', how='left').fillna(0)\n",
    "node_type_merged['coverage_pct'] = (node_type_merged['allocated_cost'] / node_type_merged['daily_cost']) * 100\n",
    "node_type_merged['difference'] = node_type_merged['daily_cost'] - node_type_merged['allocated_cost']\n",
    "\n",
    "for _, row in node_type_merged.iterrows():\n",
    "    print(f\"  {row['node_type'].upper()}:\")\n",
    "    print(f\"    Node costs: ${row['daily_cost']:,.2f}\")\n",
    "    print(f\"    Allocated: ${row['allocated_cost']:,.2f}\")\n",
    "    print(f\"    Coverage: {row['coverage_pct']:.2f}%\")\n",
    "    print(f\"    Difference: ${row['difference']:,.2f}\")\n",
    "\n",
    "# 4. By weekday/weekend\n",
    "print(\"\\nBY WEEKDAY / WEEKEND:\")\n",
    "weekday_analysis = df_daily_costs.groupby('is_weekend').agg({'daily_cost': 'sum'}).reset_index()\n",
    "weekday_allocated = df_cost_per_gen_daily.groupby('is_weekend').agg({'allocated_cost': 'sum'}).reset_index()\n",
    "weekday_merged = weekday_analysis.merge(weekday_allocated, on='is_weekend', how='left').fillna(0)\n",
    "weekday_merged['coverage_pct'] = (weekday_merged['allocated_cost'] / weekday_merged['daily_cost']) * 100\n",
    "weekday_merged['difference'] = weekday_merged['daily_cost'] - weekday_merged['allocated_cost']\n",
    "\n",
    "for _, row in weekday_merged.iterrows():\n",
    "    day_type = \"Weekend\" if row['is_weekend'] else \"Weekday\"\n",
    "    print(f\"  {day_type}:\")\n",
    "    print(f\"    Node costs: ${row['daily_cost']:,.2f}\")\n",
    "    print(f\"    Allocated: ${row['allocated_cost']:,.2f}\")\n",
    "    print(f\"    Coverage: {row['coverage_pct']:.2f}%\")\n",
    "    print(f\"    Difference: ${row['difference']:,.2f}\")\n",
    "\n",
    "# 5. By ML services\n",
    "print(\"\\nBY ML SERVICES:\")\n",
    "service_analysis = df_cost_per_gen_daily.groupby('service_name').agg({\n",
    "    'allocated_cost': 'sum',\n",
    "    'task_count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "service_analysis = service_analysis.sort_values('allocated_cost', ascending=False)\n",
    "service_analysis['cost_per_task'] = service_analysis['allocated_cost'] / service_analysis['task_count']\n",
    "\n",
    "print(\"  Top-5 services by cost:\")\n",
    "for i, (_, row) in enumerate(service_analysis.head().iterrows(), 1):\n",
    "    print(f\"    {i}. {row['service_name']}:\")\n",
    "    print(f\"       Total cost: ${row['allocated_cost']:,.2f}\")\n",
    "    print(f\"       Tasks: {row['task_count']:,}\")\n",
    "    print(f\"       Cost per task: ${row['cost_per_task']:.6f}\")\n",
    "\n",
    "# 6. Utilization efficiency\n",
    "print(\"\\nUTILIZATION EFFICIENCY:\")\n",
    "total_node_hours = df_daily_costs['node_hours'].sum()\n",
    "total_task_seconds = df_intervals['duration_seconds'].sum()\n",
    "total_task_hours = total_task_seconds / 3600\n",
    "utilization_rate = (total_task_hours / total_node_hours) * 100\n",
    "\n",
    "print(f\"  Total node runtime: {total_node_hours:,.0f} node-hours\")\n",
    "print(f\"  Total task execution time: {total_task_hours:,.0f} hours\")\n",
    "print(f\"  Utilization rate: {utilization_rate:.2f}%\")\n",
    "\n",
    "# 7. Coverage details per day\n",
    "print(\"\\nCOVERAGE TABLE (first 10 days):\")\n",
    "display_cols = ['date', 'daily_cost', 'allocated_cost', 'coverage_pct', 'difference']\n",
    "display(daily_coverage_merged[display_cols].head(10))\n",
    "\n",
    "# 8. Coverage dynamics chart\n",
    "print(\"\\nCOVERAGE DYNAMICS CHART:\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Chart 1: Node costs vs allocated costs\n",
    "ax1.plot(daily_coverage_merged['date'], daily_coverage_merged['daily_cost'],\n",
    "         label='Node costs', marker='o', linewidth=2, color='red')\n",
    "ax1.plot(daily_coverage_merged['date'], daily_coverage_merged['allocated_cost'],\n",
    "         label='Allocated costs', marker='s', linewidth=2, color='blue')\n",
    "ax1.fill_between(daily_coverage_merged['date'],\n",
    "                 daily_coverage_merged['daily_cost'],\n",
    "                 daily_coverage_merged['allocated_cost'],\n",
    "                 alpha=0.3, color='orange', label='Difference')\n",
    "ax1.set_title('Dynamics: Node costs vs Allocated costs')\n",
    "ax1.set_ylabel('Cost ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Chart 2: Coverage percentage\n",
    "ax2.plot(daily_coverage_merged['date'], daily_coverage_merged['coverage_pct'],\n",
    "         marker='o', linewidth=2, color='green')\n",
    "ax2.axhline(y=100, color='red', linestyle='--', alpha=0.7, label='100% coverage')\n",
    "ax2.axhline(y=coverage_percentage, color='orange', linestyle='--', alpha=0.7,\n",
    "            label=f'Avg. coverage: {coverage_percentage:.1f}%')\n",
    "ax2.set_title('Coverage percentage dynamics')\n",
    "ax2.set_ylabel('Coverage (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCOVERAGE ANALYSIS COMPLETED!\")\n",
    "print(\"Key findings:\")\n",
    "print(f\"   - Total coverage: {coverage_percentage:.1f}%\")\n",
    "print(f\"   - Average daily coverage: {daily_coverage_merged['coverage_pct'].mean():.1f}%\")\n",
    "print(f\"   - Max daily difference: ${daily_coverage_merged['difference'].max():,.2f}\")\n",
    "print(f\"   - Node utilization rate: {utilization_rate:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "TOTALS:\n",
    "- Node costs for entire period: `$9,400.00`\n",
    "- Allocated costs to tasks: `$9,200.00`\n",
    "- Coverage percentage: `97.87%`\n",
    "- Difference: `$200.00`\n",
    "\n",
    "DAILY COVERAGE DYNAMICS:\n",
    "- Avg. daily coverage: `97.5%`\n",
    "- Min daily coverage: `80.0%`\n",
    "- Max daily coverage: `100.0%`\n",
    "- Days with full coverage: `45`\n",
    "- Days with low coverage (<50%): `0`\n",
    "\n",
    "BY NODE TYPE:\n",
    "- A100: Node costs `$6,600.00`, Allocated `$6,500.00`, Coverage `98.5%`, Difference `$100.00`\n",
    "- L4:   Node costs `$2,800.00`, Allocated `$2,700.00`, Coverage `96.4%`, Difference `$100.00`\n",
    "\n",
    "BY WEEKDAY / WEEKEND:\n",
    "- Weekday: Node costs `$7,500.00`, Allocated `$7,400.00`, Coverage `98.7%`, Difference `$100.00`\n",
    "- Weekend: Node costs `$1,900.00`, Allocated `$1,800.00`, Coverage `94.7%`, Difference `$100.00`\n",
    "\n",
    "BY ML SERVICES (top-3):\n",
    "1. gen_image → `$3,500.00` / `1,200` tasks → `$2.92` per task\n",
    "2. gen_text  → `$2,200.00` / `900` tasks → `$2.44` per task\n",
    "3. upscale   → `$1,500.00` / `400` tasks → `$3.75` per task\n",
    "\n",
    "UTILIZATION EFFICIENCY:\n",
    "- Total node runtime: `1,200 node-hours`\n",
    "- Total task execution time: `597 hours`\n",
    "- Utilization rate: `49.8%`\n",
    "\n",
    "COVERAGE TABLE (first 5 days):\n",
    "\n",
    "| `date`     | `daily_cost` | `allocated_cost` | `coverage_pct` | `difference` |\n",
    "|------------|--------------|------------------|----------------|--------------|\n",
    "| 2025-06-01 | `256.00`     | `250.00`         | `97.7%`        | `6.00`       |\n",
    "| 2025-06-02 | `256.00`     | `255.00`         | `99.6%`        | `1.00`       |\n",
    "| 2025-06-03 | `256.00`     | `250.00`         | `97.7%`        | `6.00`       |\n",
    "| 2025-06-04 | `256.00`     | `248.00`         | `96.9%`        | `8.00`       |\n",
    "| 2025-06-05 | `256.00`     | `252.00`         | `98.4%`        | `4.00`       |\n",
    "\n",
    "COVERAGE DYNAMICS CHART:\n",
    "- Chart 1: Node costs vs allocated costs over time\n",
    "- Chart 2: Coverage percentage over time\n",
    "\n",
    "COVERAGE ANALYSIS COMPLETED!\n",
    "Key findings:\n",
    "- Total coverage: `97.9%`\n",
    "- Avg. daily coverage: `97.5%`\n",
    "- Max daily difference: `$12.00`\n",
    "- Node utilization rate: `79.2%`"
   ],
   "id": "acd57293bc0f775c"
  },
  {
   "cell_type": "markdown",
   "id": "950ce946-8b65-434e-81cd-8e937eea9abc",
   "metadata": {},
   "source": [
    "### 10.2: Detailed node utilization analysis\n",
    "\n",
    "We calculate how efficiently node resources are consumed.\n",
    "**Idle time** is defined as the sum of periods when a node was running but no tasks were executed.\n",
    "\n",
    "Important: any active node is billed regardless of actual workload."
   ]
  },
  {
   "cell_type": "code",
   "id": "8bafa24e-0baf-4bb7-b4c4-b45dd53e664d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:15:07.954519Z",
     "start_time": "2025-10-23T23:15:07.947395Z"
    }
   },
   "source": [
    "print(\"DETAILED ANALYSIS OF NODE UTILIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Hourly analysis across the entire period\n",
    "print(\"HOURLY ANALYSIS ACROSS THE ENTIRE PERIOD:\")\n",
    "\n",
    "df_cost_per_gen_daily['hour'] = df_cost_per_gen_daily['start'].dt.hour\n",
    "\n",
    "# Group by hour and day type, aggregate usage\n",
    "hourly_usage = df_cost_per_gen_daily.groupby(['hour', 'is_weekend']).agg({\n",
    "    'total_seconds': 'sum',\n",
    "    'allocated_cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Compute node-hours for the whole period for each hour\n",
    "def get_node_hours_by_hour_for_period():\n",
    "    exact_hours = []\n",
    "    start_date = pd.to_datetime('2025-06-01')\n",
    "\n",
    "    if df_daily_tasks['date'].dtype == 'object':\n",
    "        max_date = pd.to_datetime(df_daily_tasks['date'].max())\n",
    "    else:\n",
    "        max_date = df_daily_tasks['date'].max()\n",
    "\n",
    "    end_date = pd.to_datetime(max_date) if not isinstance(max_date, pd.Timestamp) else max_date\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "\n",
    "    print(f\"Analysis period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Total days: {total_days}\")\n",
    "\n",
    "    for is_weekend in [False, True]:\n",
    "        for node_type in ['a100', 'l4']:\n",
    "            subset = df_continuous[\n",
    "                (df_continuous['is_weekend'] == is_weekend) &\n",
    "                (df_continuous['node_type'] == node_type)\n",
    "            ]\n",
    "\n",
    "            for _, row in subset.iterrows():\n",
    "                hour = row['hour']\n",
    "                nodes_count = row['nodes_count']\n",
    "                days_count = int(total_days * (2/7 if is_weekend else 5/7))\n",
    "\n",
    "                exact_hours.append({\n",
    "                    'hour': hour,\n",
    "                    'is_weekend': is_weekend,\n",
    "                    'node_type': node_type,\n",
    "                    'nodes_count': nodes_count,\n",
    "                    'days_count': days_count,\n",
    "                    'node_hours_total': nodes_count * days_count\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(exact_hours)\n",
    "\n",
    "# Node-hours for the entire period\n",
    "node_schedule_period = get_node_hours_by_hour_for_period()\n",
    "hourly_schedule_period = node_schedule_period.groupby(['hour', 'is_weekend'])['node_hours_total'].sum().reset_index()\n",
    "hourly_schedule_period.columns = ['hour', 'is_weekend', 'total_node_hours_period']\n",
    "\n",
    "# Merge usage with schedule\n",
    "hourly_usage = hourly_usage.merge(hourly_schedule_period, on=['hour', 'is_weekend'], how='left').fillna(0)\n",
    "\n",
    "# Compute utilization metrics\n",
    "hourly_usage['hours_used_period'] = hourly_usage['total_seconds'] / 3600\n",
    "hourly_usage['utilization_pct'] = (\n",
    "    hourly_usage['hours_used_period'] / hourly_usage['total_node_hours_period']\n",
    ") * 100 if hourly_usage['total_node_hours_period'].sum() > 0 else 0\n",
    "\n",
    "hourly_usage_weekdays = hourly_usage[hourly_usage['is_weekend'] == False]\n",
    "hourly_usage_weekends = hourly_usage[hourly_usage['is_weekend'] == True]\n",
    "\n",
    "print(\"\\nSUMMARY FOR THE ENTIRE PERIOD:\")\n",
    "print(f\"  Total task runtime: {hourly_usage['hours_used_period'].sum():.1f} hours\")\n",
    "print(f\"  Total node runtime: {hourly_usage['total_node_hours_period'].sum():.1f} node-hours\")\n",
    "print(f\"  Overall utilization: {(hourly_usage['hours_used_period'].sum() / hourly_usage['total_node_hours_period'].sum()) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nWEEKDAYS - Top-5 busiest hours:\")\n",
    "for _, row in hourly_usage_weekdays.nlargest(5, 'hours_used_period').iterrows():\n",
    "    print(f\"    {row['hour']:02d}:00 - {row['hours_used_period']:.1f}h, {row['utilization_pct']:.1f}% (node-hours: {row['total_node_hours_period']:.1f})\")\n",
    "\n",
    "print(\"\\nWEEKDAYS - Top-5 idle hours:\")\n",
    "for _, row in hourly_usage_weekdays.nsmallest(5, 'hours_used_period').iterrows():\n",
    "    print(f\"    {row['hour']:02d}:00 - {row['hours_used_period']:.1f}h, {row['utilization_pct']:.1f}% (node-hours: {row['total_node_hours_period']:.1f})\")\n",
    "\n",
    "print(\"\\nWEEKENDS - Top-5 busiest hours:\")\n",
    "for _, row in hourly_usage_weekends.nlargest(5, 'hours_used_period').iterrows():\n",
    "    print(f\"    {row['hour']:02d}:00 - {row['hours_used_period']:.1f}h, {row['utilization_pct']:.1f}% (node-hours: {row['total_node_hours_period']:.1f})\")\n",
    "\n",
    "print(\"\\nWEEKENDS - Top-5 idle hours:\")\n",
    "for _, row in hourly_usage_weekends.nsmallest(5, 'hours_used_period').iterrows():\n",
    "    print(f\"    {row['hour']:02d}:00 - {row['hours_used_period']:.1f}h, {row['utilization_pct']:.1f}% (node-hours: {row['total_node_hours_period']:.1f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "Analysis period: 2025-06-01 to 2025-08-31\n",
    "Total days: `92`\n",
    "\n",
    "SUMMARY FOR THE ENTIRE PERIOD:\n",
    "- Total task runtime: `950.0 hours`\n",
    "- Total node runtime: `1,200.0 node-hours`\n",
    "- Overall utilization: `79.2%`\n",
    "\n",
    "WEEKDAYS - Top-5 busiest hours:\n",
    "- 14:00 → `85.0h`, `95.0%` utilization (`90.0 node-hours`)\n",
    "- 15:00 → `80.0h`, `90.0%` utilization (`89.0 node-hours`)\n",
    "- 13:00 → `75.0h`, `88.2%` utilization (`85.0 node-hours`)\n",
    "- 16:00 → `70.0h`, `84.0%` utilization (`83.0 node-hours`)\n",
    "- 12:00 → `65.0h`, `82.0%` utilization (`79.0 node-hours`)\n",
    "\n",
    "WEEKDAYS - Top-5 idle hours:\n",
    "- 03:00 → `5.0h`, `6.0%` utilization (`80.0 node-hours`)\n",
    "- 04:00 → `6.0h`, `7.5%` utilization (`80.0 node-hours`)\n",
    "- 02:00 → `8.0h`, `10.0%` utilization (`80.0 node-hours`)\n",
    "- 01:00 → `10.0h`, `12.5%` utilization (`80.0 node-hours`)\n",
    "- 05:00 → `12.0h`, `15.0%` utilization (`80.0 node-hours`)\n",
    "\n",
    "WEEKENDS - Top-5 busiest hours:\n",
    "- 20:00 → `40.0h`, `88.0%` utilization (`45.0 node-hours`)\n",
    "- 21:00 → `35.0h`, `85.0%` utilization (`41.0 node-hours`)\n",
    "- 19:00 → `32.0h`, `80.0%` utilization (`40.0 node-hours`)\n",
    "- 22:00 → `30.0h`, `78.0%` utilization (`38.0 node-hours`)\n",
    "- 18:00 → `28.0h`, `75.0%` utilization (`37.0 node-hours`)\n",
    "\n",
    "WEEKENDS - Top-5 idle hours:\n",
    "- 04:00 → `2.0h`, `5.0%` utilization (`40.0 node-hours`)\n",
    "- 03:00 → `3.0h`, `7.0%` utilization (`42.0 node-hours`)\n",
    "- 05:00 → `4.0h`, `9.5%` utilization (`42.0 node-hours`)\n",
    "- 02:00 → `5.0h`, `12.0%` utilization (`42.0 node-hours`)\n",
    "- 06:00 → `6.0h`, `14.0%` utilization (`43.0 node-hours`)"
   ],
   "id": "3ed02c142bc78d8b"
  },
  {
   "cell_type": "code",
   "id": "fa231031-0a6f-458c-8abb-78f469f32889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:16:12.856951Z",
     "start_time": "2025-10-23T23:16:12.851450Z"
    }
   },
   "source": [
    "# 2. Analysis by day of the week\n",
    "print(\"\\nANALYSIS BY DAY OF THE WEEK:\")\n",
    "\n",
    "df_cost_per_gen_daily['weekday_name'] = df_cost_per_gen_daily['start'].dt.day_name()\n",
    "\n",
    "# Group by weekday and aggregate usage\n",
    "weekday_hours = df_cost_per_gen_daily.groupby('weekday_name').agg({\n",
    "    'total_seconds': 'sum',\n",
    "    'allocated_cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "weekday_hours['hours_used'] = weekday_hours['total_seconds'] / 3600\n",
    "\n",
    "# Map weekday names to numbers (0=Monday, 6=Sunday)\n",
    "weekday_mapping = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "    'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "}\n",
    "\n",
    "weekday_hours['weekday_num'] = weekday_hours['weekday_name'].map(weekday_mapping)\n",
    "\n",
    "# Get node-hours per weekday across the entire period\n",
    "weekday_node_hours_df = df_daily_costs.copy()\n",
    "weekday_node_hours_df['weekday_name'] = pd.to_datetime(weekday_node_hours_df['date']).dt.day_name()\n",
    "\n",
    "weekday_node_hours_df = weekday_node_hours_df.groupby(['weekday_name', 'is_weekend']).agg({\n",
    "    'node_hours': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "weekday_node_hours_df['weekday_num'] = weekday_node_hours_df['weekday_name'].map(weekday_mapping)\n",
    "\n",
    "# Merge with usage data\n",
    "weekday_hours = weekday_hours.merge(weekday_node_hours_df, on='weekday_num', how='left')\n",
    "\n",
    "# Calculate utilization percentage per weekday\n",
    "weekday_hours['utilization_pct'] = (\n",
    "    weekday_hours['hours_used'] / weekday_hours['node_hours']\n",
    ") * 100 if weekday_hours['node_hours'].sum() > 0 else 0\n",
    "\n",
    "# Sort by weekday number for correct order\n",
    "weekday_hours = weekday_hours.sort_values('weekday_num')\n",
    "\n",
    "print(\"\\nUTILIZATION ANALYSIS BY WEEKDAY (FULL PERIOD):\")\n",
    "for _, row in weekday_hours.iterrows():\n",
    "    day_type = \"Weekend\" if row['is_weekend'] else \"Weekday\"\n",
    "    print(f\"  {row['weekday_name']} ({day_type}):\")\n",
    "    print(f\"    Task runtime: {row['hours_used']:.1f}h\")\n",
    "    print(f\"    Node-hours: {row['node_hours']:.1f}\")\n",
    "    print(f\"    Utilization: {row['utilization_pct']:.1f}%\\n\")\n",
    "\n",
    "# Show summary table\n",
    "print(\"\\nSUMMARY TABLE BY WEEKDAY:\")\n",
    "display_cols = ['weekday_name', 'is_weekend', 'hours_used', 'node_hours', 'utilization_pct']\n",
    "display(weekday_hours[display_cols].round(2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "UTILIZATION ANALYSIS BY WEEKDAY (FULL PERIOD):\n",
    "\n",
    "  Monday (Weekday):\n",
    "    Task runtime: 120.0h\n",
    "    Node-hours: 200.0\n",
    "    Utilization: 60.0%\n",
    "\n",
    "  Tuesday (Weekday):\n",
    "    Task runtime: 125.0h\n",
    "    Node-hours: 200.0\n",
    "    Utilization: 62.5%\n",
    "\n",
    "  Wednesday (Weekday):\n",
    "    Task runtime: 115.0h\n",
    "    Node-hours: 200.0\n",
    "    Utilization: 57.5%\n",
    "\n",
    "  Thursday (Weekday):\n",
    "    Task runtime: 130.0h\n",
    "    Node-hours: 200.0\n",
    "    Utilization: 65.0%\n",
    "\n",
    "  Friday (Weekday):\n",
    "    Task runtime: 110.0h\n",
    "    Node-hours: 200.0\n",
    "    Utilization: 55.0%\n",
    "\n",
    "  Saturday (Weekend):\n",
    "    Task runtime: 70.0h\n",
    "    Node-hours: 180.0\n",
    "    Utilization: 38.9%\n",
    "\n",
    "  Sunday (Weekend):\n",
    "    Task runtime: 75.0h\n",
    "    Node-hours: 180.0\n",
    "    Utilization: 41.7%\n",
    "\n",
    "\n",
    "SUMMARY TABLE BY WEEKDAY:\n",
    "\n",
    "| `weekday_name` | `is_weekend` | `hours_used` | `node_hours` | `utilization_pct` |\n",
    "|----------------|--------------|--------------|--------------|-------------------|\n",
    "| Monday         | False        | 120.0        | 200.0        | 60.0              |\n",
    "| Tuesday        | False        | 125.0        | 200.0        | 62.5              |\n",
    "| Wednesday      | False        | 115.0        | 200.0        | 57.5              |\n",
    "| Thursday       | False        | 130.0        | 200.0        | 65.0              |\n",
    "| Friday         | False        | 110.0        | 200.0        | 55.0              |\n",
    "| Saturday       | True         | 70.0         | 180.0        | 38.9              |\n",
    "| Sunday         | True         | 75.0         | 180.0        | 41.7              |"
   ],
   "id": "2f5bd13e4fd3c2bb"
  },
  {
   "cell_type": "code",
   "id": "39fe9055-81e4-49c9-93d7-55b3a69a1fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:19:21.350904Z",
     "start_time": "2025-10-23T23:19:21.346316Z"
    }
   },
   "source": [
    "# 3. Idle time analysis\n",
    "print(\"\\nIDLE TIME ANALYSIS:\")\n",
    "total_node_hours = df_daily_costs['node_hours'].sum()\n",
    "total_task_hours = df_daily_tasks['total_seconds'].sum() / 3600\n",
    "idle_hours = total_node_hours - total_task_hours\n",
    "\n",
    "print(f\"  Total node runtime: {total_node_hours:,.0f} node-hours\")\n",
    "print(f\"  Task runtime: {total_task_hours:,.0f} hours\")\n",
    "print(f\"  Idle time: {idle_hours:,.0f} hours\")\n",
    "print(f\"  Idle percentage: {(idle_hours / total_node_hours) * 100:.1f}%\")\n",
    "\n",
    "# 4. Analysis by node type\n",
    "print(\"\\nANALYSIS BY NODE TYPE:\")\n",
    "node_type_efficiency = df_daily_costs.groupby('node_type').agg({\n",
    "    'node_hours': 'sum',\n",
    "    'daily_cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "node_type_tasks = df_cost_per_gen_daily.groupby('node_type').agg({\n",
    "    'allocated_cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "node_type_efficiency = node_type_efficiency.merge(node_type_tasks, on='node_type', how='left').fillna(0)\n",
    "node_type_efficiency['cost_per_hour'] = node_type_efficiency['daily_cost'] / node_type_efficiency['node_hours']\n",
    "node_type_efficiency['efficiency_ratio'] = node_type_efficiency['allocated_cost'] / node_type_efficiency['daily_cost']\n",
    "\n",
    "for _, row in node_type_efficiency.iterrows():\n",
    "    print(f\"  {row['node_type'].upper()}:\")\n",
    "    print(f\"    Cost per hour: ${row['cost_per_hour']:.4f}\")\n",
    "    print(f\"    Efficiency: {row['efficiency_ratio']*100:.1f}%\")\n",
    "\n",
    "# 5. Gap analysis\n",
    "print(\"\\nTASK GAP ANALYSIS:\")\n",
    "df_intervals_sorted = df_intervals.sort_values(['date', 'start'])\n",
    "df_intervals_sorted['next_start'] = df_intervals_sorted.groupby('date')['start'].shift(-1)\n",
    "df_intervals_sorted['gap_hours'] = (df_intervals_sorted['next_start'] - df_intervals_sorted['end']).dt.total_seconds() / 3600\n",
    "\n",
    "gaps = df_intervals_sorted[df_intervals_sorted['gap_hours'] > 0]\n",
    "if not gaps.empty:\n",
    "    print(f\"  Avg. gap between tasks: {gaps['gap_hours'].mean():.2f} hours\")\n",
    "    print(f\"  Max gap: {gaps['gap_hours'].max():.2f} hours\")\n",
    "    print(f\"  Total number of gaps: {len(gaps)}\")\n",
    "else:\n",
    "    print(\"  Tasks run continuously\")\n",
    "\n",
    "# 6. Optimization recommendations\n",
    "print(\"\\nOPTIMIZATION RECOMMENDATIONS:\")\n",
    "\n",
    "night_hours = hourly_usage[hourly_usage['hour'].isin([0,1,2,3,4,5,6])]\n",
    "night_utilization = night_hours['utilization_pct'].mean()\n",
    "print(f\"  Nighttime utilization (0-6h): {night_utilization:.1f}%\")\n",
    "if night_utilization < 10:\n",
    "    print(\"    Low nighttime utilization - consider shutting down nodes\")\n",
    "elif night_utilization < 30:\n",
    "    print(\"    Moderate nighttime utilization - consider partial shutdown\")\n",
    "\n",
    "weekend_utilization = weekday_hours[weekday_hours['weekday_name'].isin(['Saturday', 'Sunday'])]['utilization_pct'].mean()\n",
    "print(f\"  Weekend utilization: {weekend_utilization:.1f}%\")\n",
    "if weekend_utilization < 20:\n",
    "    print(\"    Low weekend utilization - consider shutting down nodes\")\n",
    "\n",
    "# 7. Visualization\n",
    "print(\"\\nVISUALIZATION OF HOURLY AND WEEKDAY UTILIZATION:\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "ax1.bar(hourly_usage['hour'], hourly_usage['utilization_pct'], color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "ax1.set_title('Node utilization by hour of day')\n",
    "ax1.set_xlabel('Hour of day')\n",
    "ax1.set_ylabel('Utilization (%)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(range(0, 24))\n",
    "\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_usage_ordered = weekday_hours.set_index('weekday_name').reindex(weekday_order)\n",
    "\n",
    "ax2.bar(range(len(weekday_usage_ordered)), weekday_usage_ordered['utilization_pct'],\n",
    "        color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "ax2.set_title('Node utilization by weekday')\n",
    "ax2.set_xlabel('Weekday')\n",
    "ax2.set_ylabel('Utilization (%)')\n",
    "ax2.set_xticks(range(len(weekday_usage_ordered)))\n",
    "ax2.set_xticklabels(weekday_usage_ordered.index, rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Detailed hourly stats\n",
    "hourly_detailed = hourly_usage.copy()\n",
    "hourly_detailed['cost_per_hour'] = hourly_detailed['allocated_cost'] / hourly_detailed['hours_used_period']\n",
    "hourly_detailed['efficiency_score'] = hourly_detailed['utilization_pct'] / hourly_detailed['utilization_pct'].max()\n",
    "\n",
    "print(\"\\nUTILIZATION ANALYSIS COMPLETED!\")\n",
    "print(\"Key insights:\")\n",
    "print(f\"   - Busiest hour: {hourly_usage.loc[hourly_usage['utilization_pct'].idxmax(), 'hour']:02d}:00\")\n",
    "print(f\"   - Idlest hour: {hourly_usage.loc[hourly_usage['utilization_pct'].idxmin(), 'hour']:02d}:00\")\n",
    "print(f\"   - Busiest day: {weekday_hours.loc[weekday_hours['utilization_pct'].idxmax(), 'weekday_name']}\")\n",
    "print(f\"   - Potential savings: {idle_hours:.0f} idle hours\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "IDLE TIME ANALYSIS:\n",
    "  Total node runtime: 12,000 node-hours\n",
    "  Task runtime: 6,000 hours\n",
    "  Idle time: 6,000 hours\n",
    "  Idle percentage: 50.0%\n",
    "\n",
    "ANALYSIS BY NODE TYPE:\n",
    "  A100:\n",
    "    Cost per hour: $11.0000\n",
    "    Efficiency: 52.0%\n",
    "  L4:\n",
    "    Cost per hour: $1.0000\n",
    "    Efficiency: 48.0%\n",
    "\n",
    "TASK GAP ANALYSIS:\n",
    "  Avg. gap between tasks: 0.80 hours\n",
    "  Max gap: 5.20 hours\n",
    "  Total number of gaps: 430\n",
    "\n",
    "OPTIMIZATION RECOMMENDATIONS:\n",
    "  Nighttime utilization (0-6h): 8.5%\n",
    "    Low nighttime utilization - consider shutting down nodes\n",
    "  Weekend utilization: 40.0%\n",
    "\n",
    "VISUALIZATION OF HOURLY AND WEEKDAY UTILIZATION:\n",
    "(Hourly bar chart + weekday bar chart)\n",
    "\n",
    "Key insights:\n",
    "   - Busiest hour: 14:00\n",
    "   - Idlest hour: 04:00\n",
    "   - Busiest day: Wednesday\n",
    "   - Potential savings: 6,000 idle hours"
   ],
   "id": "c8da7e8060e62bd9"
  },
  {
   "cell_type": "markdown",
   "id": "9b9f88d2",
   "metadata": {},
   "source": [
    "## Step 11: Final Tables and Statistics\n",
    "\n",
    "We generate final summary tables with analysis results:\n",
    "1. **Summary by ML services** – generation costs per service\n",
    "2. **Summary by node types** – overall statistics for A100 and L4\n",
    "3. **Split statistics** – successful vs failed tasks\n",
    "4. **Top services** by different metrics\n",
    "\n",
    "**Key metrics**:\n",
    "- Successful tasks: cost per generation\n",
    "- Failed tasks: cost per second"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:20:33.079234Z",
     "start_time": "2025-10-23T23:20:33.065306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final summary tables\n",
    "print(\"FINAL ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Helper: calculate distribution stats\n",
    "def calc_stats(series, weights=None):\n",
    "    if series.empty:\n",
    "        return {}\n",
    "    return {\n",
    "        \"count\": len(series),\n",
    "        \"mean\": series.mean(),\n",
    "        \"median\": series.median(),\n",
    "        \"p05\": series.quantile(0.05),\n",
    "        \"p25\": series.quantile(0.25),\n",
    "        \"p75\": series.quantile(0.75),\n",
    "        \"p90\": series.quantile(0.90),\n",
    "        \"p95\": series.quantile(0.95),\n",
    "        \"weighted_mean\": (series * weights).sum() / weights.sum() if weights is not None else None\n",
    "    }\n",
    "\n",
    "# Table 0: Summary by all ML services\n",
    "print(\"\\n=== TABLE 0: SUMMARY BY ALL ML SERVICES ===\")\n",
    "all_services = df_cost_per_gen_daily\n",
    "\n",
    "summary_by_service = all_services.groupby('service_name').agg({\n",
    "    'task_count': 'sum',\n",
    "    'allocated_cost': 'sum',\n",
    "    'cost_per_generation': ['mean', 'std']\n",
    "}).round(6)\n",
    "\n",
    "summary_by_service.columns = ['total_tasks', 'total_allocated_cost', 'avg_cost_per_gen', 'std_cost_per_gen']\n",
    "\n",
    "summary_by_service['weighted_avg_cost'] = (\n",
    "    summary_by_service['total_allocated_cost'] / summary_by_service['total_tasks']\n",
    ").round(6)\n",
    "\n",
    "summary_by_service = summary_by_service.sort_values('total_tasks', ascending=False)\n",
    "display(summary_by_service.head(15))\n",
    "\n",
    "# Table 1: Successful tasks only\n",
    "print(\"\\n=== TABLE 1: SUMMARY BY ML SERVICES (SUCCESSFUL) ===\")\n",
    "success_services = df_cost_per_gen_daily[df_cost_per_gen_daily['success'] & df_cost_per_gen_daily['cost_per_generation'].notna()]\n",
    "\n",
    "summary_by_service_success = success_services.groupby('service_name').agg({\n",
    "    'task_count': 'sum',\n",
    "    'allocated_cost': 'sum',\n",
    "    'cost_per_generation': ['mean', 'std']\n",
    "}).round(6)\n",
    "\n",
    "summary_by_service_success.columns = ['total_tasks', 'total_allocated_cost', 'avg_cost_per_gen', 'std_cost_per_gen']\n",
    "\n",
    "summary_by_service_success['weighted_avg_cost'] = (\n",
    "    summary_by_service_success['total_allocated_cost'] / summary_by_service_success['total_tasks']\n",
    ").round(6)\n",
    "\n",
    "summary_by_service_success = summary_by_service_success.sort_values('total_tasks', ascending=False)\n",
    "display(summary_by_service_success.head(15))\n",
    "\n",
    "# Table 2: Failed tasks only\n",
    "print(\"\\n=== TABLE 2: SUMMARY BY ML SERVICES (FAILED) ===\")\n",
    "fail_services = df_cost_per_gen_daily[~df_cost_per_gen_daily['success'] & df_cost_per_gen_daily['cost_per_second'].notna()]\n",
    "\n",
    "summary_by_service_fail = fail_services.groupby('service_name').agg({\n",
    "    'task_count': 'sum',\n",
    "    'total_duration_seconds': 'sum',\n",
    "    'allocated_cost': 'sum',\n",
    "    'cost_per_second': ['mean', 'std']\n",
    "}).round(6)\n",
    "\n",
    "summary_by_service_fail.columns = ['total_tasks', 'total_seconds', 'total_allocated_cost', 'avg_cost_per_second', 'std_cost_per_second']\n",
    "\n",
    "summary_by_service_fail['weighted_avg_cost_per_second'] = (\n",
    "    summary_by_service_fail['total_allocated_cost'] / summary_by_service_fail['total_seconds']\n",
    ").round(6)\n",
    "\n",
    "summary_by_service_fail = summary_by_service_fail.sort_values('total_tasks', ascending=False)\n",
    "display(summary_by_service_fail.head(15))\n",
    "\n",
    "# Table 3: Summary by node type\n",
    "print(\"\\n=== TABLE 3: SUMMARY BY NODE TYPE ===\")\n",
    "summary_by_node_type = df_cost_per_gen_daily.groupby('node_type').agg({\n",
    "    'task_count': 'sum',\n",
    "    'allocated_cost': 'sum'\n",
    "}).round(6)\n",
    "\n",
    "summary_by_node_type['total_cost'] = summary_by_node_type['allocated_cost']\n",
    "display(summary_by_node_type)\n",
    "\n",
    "# Global stats\n",
    "print(\"\\n=== GLOBAL STATS ===\")\n",
    "total_tasks_all = df_cost_per_gen_daily['task_count'].sum()\n",
    "total_cost_all = df_cost_per_gen_daily['allocated_cost'].sum()\n",
    "\n",
    "print(f\"Total tasks: {total_tasks_all:,}\")\n",
    "print(f\"Total node costs: ${total_cost_all:.2f}\")\n",
    "print(f\"Analysis period: from 2025-06-01 to {datetime.now().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Cost distribution stats\n",
    "success_costs = df_cost_per_gen_daily.loc[df_cost_per_gen_daily.success & df_cost_per_gen_daily.cost_per_generation.notna(), \"cost_per_generation\"]\n",
    "fail_costs = df_cost_per_gen_daily.loc[~df_cost_per_gen_daily.success & df_cost_per_gen_daily.cost_per_second.notna(), \"cost_per_second\"]\n",
    "\n",
    "print(\"\\n=== COST DISTRIBUTION STATS ===\")\n",
    "print(\"Successful tasks:\")\n",
    "print(calc_stats(success_costs))\n",
    "\n",
    "print(\"\\nFailed tasks (cost per second):\")\n",
    "print(calc_stats(fail_costs))"
   ],
   "id": "9c6d9de4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock output (illustrative example):**\n",
    "\n",
    "=== TABLE 0: SUMMARY BY ALL ML SERVICES ===\n",
    "| service_name    | total_tasks | total_allocated_cost | avg_cost_per_gen | std_cost_per_gen | weighted_avg_cost |\n",
    "|-----------------|-------------|----------------------|------------------|------------------|-------------------|\n",
    "| img_gen_v2      | 45,000      | 250,000.00           | 0.0060           | 0.0012           | 0.0056            |\n",
    "| upscale_ai      | 12,500      | 80,000.00            | 0.0072           | 0.0021           | 0.0064            |\n",
    "| text2img_beta   | 8,200       | 42,000.00            | 0.0054           | 0.0018           | 0.0051            |\n",
    "\n",
    "=== TABLE 1: SUMMARY BY ML SERVICES (SUCCESSFUL) ===\n",
    "(similar structure, only successful tasks included)\n",
    "\n",
    "=== TABLE 2: SUMMARY BY ML SERVICES (FAILED) ===\n",
    "| service_name    | total_tasks | total_seconds | total_allocated_cost | avg_cost_per_second | std_cost_per_second | weighted_avg_cost_per_second |\n",
    "|-----------------|-------------|---------------|----------------------|---------------------|---------------------|-------------------------------|\n",
    "| img_gen_v2      | 4,000       | 180,000       | 20,000.00            | 0.1200              | 0.0450              | 0.1111                        |\n",
    "| upscale_ai      | 1,200       | 40,000        | 3,800.00             | 0.0950              | 0.0310              | 0.0945                        |\n",
    "\n",
    "=== TABLE 3: SUMMARY BY NODE TYPE ===\n",
    "| node_type | task_count | allocated_cost | total_cost |\n",
    "|-----------|------------|----------------|------------|\n",
    "| a100      | 38,000     | 220,000.00     | 220,000.00 |\n",
    "| l4        | 31,000     | 152,000.00     | 152,000.00 |\n",
    "\n",
    "=== GLOBAL STATS ===\n",
    "- Total tasks: 69,000\n",
    "- Total node costs: $372,000.00\n",
    "- Analysis period: from 2025-06-01 to 2025-10-23\n",
    "\n",
    "=== COST DISTRIBUTION STATS ===\n",
    "Successful tasks:\n",
    "`{'count': 65,000, 'mean': 0.0061, 'median': 0.0058, 'p05': 0.0030, 'p25': 0.0045, 'p75': 0.0074, 'p90': 0.0091, 'p95': 0.0100, 'weighted_mean': 0.0059}`\n",
    "\n",
    "Failed tasks (cost per second):\n",
    "`{'count': 4,000, 'mean': 0.112, 'median': 0.105, 'p05': 0.050, 'p25': 0.080, 'p75': 0.135, 'p90': 0.150, 'p95': 0.170, 'weighted_mean': 0.111}`"
   ],
   "id": "69a85e82aab04a04"
  },
  {
   "cell_type": "markdown",
   "id": "edc2863b",
   "metadata": {},
   "source": [
    "## Step 11: Results Visualization\n",
    "\n",
    "We create visualizations to present the analysis results more clearly:\n",
    "1. **Daily dynamics of successful generation costs**\n",
    "2. **Daily dynamics of failed task costs**\n",
    "3. **Cost comparison by node type**\n",
    "4. **Top services by total cost**"
   ]
  },
  {
   "cell_type": "code",
   "id": "daaf41d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:22:17.750731Z",
     "start_time": "2025-10-23T23:22:17.747467Z"
    }
   },
   "source": [
    "# Create visualizations\n",
    "print(\"Creating plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Dynamics of average successful generation cost\n",
    "success_by_date = df_cost_per_gen_daily[df_cost_per_gen_daily.success].groupby(\"date\")[\"cost_per_generation\"].mean()\n",
    "success_by_date.plot(kind='line', marker=\"o\", ax=axes[0,0], color='green', linewidth=2)\n",
    "axes[0,0].set_title('Dynamics of Avg. Successful Generation Cost\\n(from 2025-06-01)')\n",
    "axes[0,0].set_ylabel('Cost per generation ($)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Dynamics of average failed task cost (per second)\n",
    "fail_by_date = df_cost_per_gen_daily[~df_cost_per_gen_daily.success].groupby(\"date\")[\"cost_per_second\"].mean()\n",
    "fail_by_date.plot(kind='line', marker=\"o\", ax=axes[0,1], color=\"red\", linewidth=2)\n",
    "axes[0,1].set_title('Dynamics of Avg. Failed Task Cost\\n(cost per second)')\n",
    "axes[0,1].set_ylabel('Cost per second ($)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Total cost by node type\n",
    "node_type_costs = df_cost_per_gen_daily.groupby('node_type')['allocated_cost'].sum()\n",
    "node_type_costs.plot(kind='bar', ax=axes[1,0], color=['#ff7f0e', '#1f77b4'])\n",
    "axes[1,0].set_title('Total Cost by Node Type')\n",
    "axes[1,0].set_ylabel('Total cost ($)')\n",
    "axes[1,0].tick_params(axis='x', rotation=0)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Top-10 services by weighted avg cost (successful generations)\n",
    "top_10_success = summary_by_service_success.head(10)\n",
    "top_10_success['weighted_avg_cost'].plot(kind='bar', ax=axes[1,1], color='lightgreen')\n",
    "axes[1,1].set_title('Top-10 Services by Successful Generation Cost')\n",
    "axes[1,1].set_ylabel('Cost per generation ($)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalysis completed!\")\n",
    "print(\"All summary tables and plots created\")\n",
    "print(\"Node costs allocated proportionally to runtime\")\n",
    "print(\"Separate pricing applied: per-task for successful, per-second for failed\")\n",
    "print(\"Weighted averages shown for each ML service\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Mock visualizations:**\n",
    "\n",
    "1. **Dynamics of Avg. Successful Generation Cost**\n",
    "   - Smooth line around $0.006–0.007 per generation\n",
    "   - Slight downward trend as autoscaling reduces idle costs\n",
    "\n",
    "2. **Dynamics of Avg. Failed Task Cost (per second)**\n",
    "   - Fluctuates more, around $0.09–0.13 per second\n",
    "   - Occasional spikes when failures cluster\n",
    "\n",
    "3. **Total Cost by Node Type**\n",
    "   - A100: ~$220,000\n",
    "   - L4: ~$152,000\n",
    "\n",
    "4. **Top-10 Services by Successful Generation Cost**\n",
    "   - `img_gen_v2`: ~$0.0056 per generation\n",
    "   - `upscale_ai`: ~$0.0064 per generation\n",
    "   - `text2img_beta`: ~$0.0051 per generation\n",
    "   - (others descending…)"
   ],
   "id": "50df21e1ab6af14d"
  },
  {
   "cell_type": "markdown",
   "id": "0c443345",
   "metadata": {},
   "source": [
    "## Saving Results\n",
    "\n",
    "Save the main analysis outputs into CSV files for further use."
   ]
  },
  {
   "cell_type": "code",
   "id": "cf8a804d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:23:48.809083Z",
     "start_time": "2025-10-23T23:23:48.803810Z"
    }
   },
   "source": [
    "# Save results\n",
    "try:\n",
    "    # All tasks\n",
    "    summary_by_service.to_csv('../data/gen_price_by_service_enhanced.csv')\n",
    "    print(\"Saved: gen_price_by_service_enhanced.csv\")\n",
    "\n",
    "    # Successful tasks\n",
    "    summary_by_service_success.to_csv('../data/gen_price_by_service_success_enhanced.csv')\n",
    "    print(\"Saved: gen_price_by_service_success_enhanced.csv\")\n",
    "\n",
    "    # Failed tasks\n",
    "    summary_by_service_fail.to_csv('../data/gen_price_by_service_fail_enhanced.csv')\n",
    "    print(\"Saved: gen_price_by_service_fail_enhanced.csv\")\n",
    "\n",
    "    # Detailed daily costs per generation\n",
    "    df_cost_per_gen_daily.to_csv('../data/gen_cost_detailed_enhanced.csv', index=False)\n",
    "    print(\"Saved: gen_cost_detailed_enhanced.csv\")\n",
    "\n",
    "    # Daily Node Costs\n",
    "    df_daily_costs.to_csv('../data/daily_costs_june-aug.csv', index=False)\n",
    "    print(\"Saved: daily_costs_june-aug.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while saving: {e}\")\n",
    "\n",
    "print(\"\\nAnalysis fully completed!\")\n",
    "print(\"Results saved in /data/ folder\")\n",
    "print(\"Enhanced methodology applied for runtime calculation\")\n",
    "print(\"Separate pricing implemented: per-task for successful, per-second for failed tasks\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99fe6285",
   "metadata": {},
   "source": [
    "## Additional: Export Raw Task Data\n",
    "\n",
    "We generate a detailed table for each task with key metrics:\n",
    "- Task date\n",
    "- Node type (A100 / L4)\n",
    "- Service\n",
    "- Success / failure\n",
    "- Runtime duration (seconds)\n",
    "- Allocated computation cost\n",
    "- Weighted average cost"
   ]
  },
  {
   "cell_type": "code",
   "id": "139e3294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T23:24:43.565300Z",
     "start_time": "2025-10-23T23:24:43.554586Z"
    }
   },
   "source": [
    "# Create a detailed task-level table based on df_intervals\n",
    "df_tasks_raw = df_intervals.copy()\n",
    "\n",
    "# Function to enrich task data with cost info\n",
    "def add_cost_info_to_tasks(df_tasks_raw, df_cost_per_gen_daily):\n",
    "    tasks_with_cost = []\n",
    "\n",
    "    for _, task_row in df_tasks_raw.iterrows():\n",
    "        task_date = task_row['date']\n",
    "        service_name = task_row['service_name']\n",
    "        node_type = task_row['node_type']\n",
    "        success = task_row['success']\n",
    "        duration_seconds = task_row['duration_seconds']\n",
    "\n",
    "        # Match corresponding cost entry\n",
    "        cost_row = df_cost_per_gen_daily[\n",
    "            (df_cost_per_gen_daily['date'] == task_date) &\n",
    "            (df_cost_per_gen_daily['service_name'] == service_name) &\n",
    "            (df_cost_per_gen_daily['node_type'] == node_type) &\n",
    "            (df_cost_per_gen_daily['success'] == success)\n",
    "        ]\n",
    "\n",
    "        if len(cost_row) > 0:\n",
    "            cost_info = cost_row.iloc[0]\n",
    "\n",
    "            if success:\n",
    "                # Successful tasks: cost per generation\n",
    "                allocated_cost = cost_info['cost_per_generation']\n",
    "                weighted_avg_cost = cost_info['cost_per_generation']\n",
    "            else:\n",
    "                # Failed tasks: cost per second × duration\n",
    "                cost_per_second = cost_info['cost_per_second']\n",
    "                allocated_cost = cost_per_second * duration_seconds if cost_per_second else 0\n",
    "                weighted_avg_cost = cost_per_second\n",
    "\n",
    "            tasks_with_cost.append({\n",
    "                'task_id': task_row['task_id'],\n",
    "                'task_date': task_date,\n",
    "                'node_type': node_type,\n",
    "                'service_name': service_name,\n",
    "                'success': success,\n",
    "                'duration_seconds': duration_seconds,\n",
    "                'allocated_cost_per_task': allocated_cost,\n",
    "                'weighted_avg_cost': weighted_avg_cost,\n",
    "                'cost_type': 'per_task' if success else 'per_second'\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(tasks_with_cost)\n",
    "\n",
    "# Generate the detailed table\n",
    "df_tasks_detailed = add_cost_info_to_tasks(df_tasks_raw, df_cost_per_gen_daily)\n",
    "\n",
    "print(f\"Detailed task table created: {len(df_tasks_detailed)} rows\")\n",
    "\n",
    "if len(df_tasks_detailed) > 0:\n",
    "    print(\"\\n=== STRUCTURE OF DETAILED TABLE ===\")\n",
    "    display(df_tasks_detailed.head(10))\n",
    "\n",
    "    print(f\"\\n=== STATS ===\")\n",
    "    print(f\"Total tasks: {len(df_tasks_detailed):,}\")\n",
    "    print(f\"Successful: {df_tasks_detailed['success'].sum():,}\")\n",
    "    print(f\"Failed: {(~df_tasks_detailed['success']).sum():,}\")\n",
    "    print(f\"Total allocated cost: ${df_tasks_detailed['allocated_cost_per_task'].sum():.2f}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    try:\n",
    "        df_tasks_detailed.to_csv('../data/tasks_raw_detailed_enhanced.csv', index=False)\n",
    "        print(\"\\nSaved: tasks_raw_detailed_enhanced.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError while saving: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
